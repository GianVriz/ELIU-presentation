labs(title = 'Satellite', x = "Longitude", y = "Latitude") +
annotation_north_arrow(location = "bl", which_north = "true",
pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
style = north_arrow_fancy_orienteering)+
scale_color_gradientn( colours = rainbow(3), name = "Wet days")
map_1
map_1
map-4
map_4
theme_update(plot.title = element_text(hjust = 0.5))
map_4 <- ggplot() +
geom_map(
data = world, map = world,
aes(long, lat, map_id = region),
color = "black", fill = "lightgray", size = 0.3
) +
geom_point(
data = station_2018,
aes(Longitude, Latitude, color= Count), size = 1
)+
labs(title = 'World stations', x = "Longitude", y = "Latitude") +
annotation_north_arrow(location = "bl", which_north = "true",
pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
style = north_arrow_fancy_orienteering)+
scale_color_gradientn(limits = c(0,365),colours = rainbow(3), name = "Wet days")
map_4
map_4
load("C:/Users/vrizg/Desktop/Statistical Consulting/Estimates/Data_2000_2018.RData")
library(readr)
library(xgboost)
library(dplyr)
library(data.table)
library(mgcv)
library(ggplot2)
library(R.matlab)
library(ggspatial)
library(EnvStats)
library(Metrics)
library(tseries)
library(raster)
library(sp)
library(keras)
library(plyr)
all_3 <- all_2 %>% dplyr::select(4,7,8,9,10,14,15,16,17)
all_3[is.na(all_3)] <- 0
#make this example reproducible
set.seed(1)
#use 80% of dataset as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(all_3), replace=TRUE, prob=c(0.8,0.2))
train <- all_3[sample, ]
test <- all_3[!sample, ]
cat("RMSE:", rmse(test$Count.y, test$Count.x))
xtrain = data.matrix(train[,-4])
ytrain = train[,4]
xtest = data.matrix(test[,-4])
ytest = test[,4]
xgb_train = xgb.DMatrix(data = xtrain, label = ytrain)
xgb_test = xgb.DMatrix(data = xtest, label = ytest)
watchlist = list(train=xgb_train, test=xgb_test)
#create hyperparameter grid
max.depths = c(2, 3, 5, 6, 7)
etas = c(0.01, 0.02, 0.35)
best_params = 0
best_score = 0
count = 1
for( depth in max.depths ){
for( num in etas){
bst_grid = xgb.train(data = xgb_train,
max.depth = depth,
eta=num,
nthread = 2,
nround = 1000,
watchlist = watchlist,
objective = "reg:squarederror",
early_stopping_rounds = 50,
verbose=0)
if(count == 1){
best_params = bst_grid$params
best_score = bst_grid$best_score
count = count + 1
}
else if( bst_grid$best_score < best_score){
best_params = bst_grid$params
best_score = bst_grid$best_score
}
}
}
best_params
best_score
#fit XGBoost model and display training and testing data at each iteartion
#xgb.train(data = xgb_train, max.depth = 2, watchlist=watchlist, nrounds = 1000)
model_xgb = xgb.train(data = xgb_train, nthread = 2, eta = 0.02, max.depth = 6, nrounds = 1000, watchlist = watchlist, objective = "reg:squarederror", early_stopping_rounds = 50,  verbose = 1)
summary(model_xgb)
pred_xgb = predict(model_xgb, xgb_test)
cat("RMSE:", rmse(ytest, pred_xgb))
x = 1:length(ytest)                   # visualize the model, actual and predicted data
plot(x, ytest, col = "blue", type = "l", xlab='Stations',ylab = 'Average number of wet days')
lines(x, pred_xgb, col = "red", type = "l")
legend("topleft", legend = c("y-test", "y-pred"),
col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
importance <- xgb.importance(feature_names = colnames(xgb_test),model = model_xgb)
print(xgb.ggplot.importance(importance_matrix = importance))
all_3 <- all_2 %>% dplyr::select(3,6,7,8,4,10,14,15,16,17)
#all_3$Count.x<-Count.pred
all_3[is.na(all_3)] <- 0
#make this example reproducible
set.seed(1)
#use 80% of dataset as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(all_3), replace=TRUE, prob=c(0.8,0.2))
train <- all_3[sample, ]
test <- all_3[!sample, ]
all_3 <- all_2 %>% dplyr::select(3,6,7,8,4,10,14,15,16,17)
#all_3$Count.x<-Count.pred
all_3[is.na(all_3)] <- 0
#make this example reproducible
set.seed(1)
#use 80% of dataset as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(all_3), replace=TRUE, prob=c(0.8,0.2))
train <- all_3[sample, ]
test <- all_3[!sample, ]
cat("RMSE:", rmse(test$Scale.y, test$Scale.x))
reg_l<-lm(Scale.y~Scale.x+Longitude+bio1, data=train)
summary(reg_l)
pred_reg <- predict(reg_l,test)
cat("RMSE:", rmse(test$Scale.y, pred_reg))
#tunning parameters
xtrain = data.matrix(train[,-2])
ytrain = train[,2]
xtest = data.matrix(test[,-2])
ytest = test[,2]
xgb_train = xgb.DMatrix(data = xtrain, label = ytrain)
xgb_test = xgb.DMatrix(data = xtest, label = ytest)
watchlist = list(train=xgb_train, test=xgb_test)
#create hyperparameter grid
max.depths = c(2, 3, 4, 5, 6, 7, 10)
etas = c(0.01, 0.02, 0.1, 0.2, 0.3, 0.32, 0.35, 0.37, 0.38, 0.4, 0.5)
best_params = 0
best_score = 0
count = 1
for( depth in max.depths ){
for( num in etas){
bst_grid = xgb.train(data = xgb_train,
max.depth = depth,
eta=num,
nthread = 2,
nround = 1000,
watchlist = watchlist,
objective = "reg:squarederror",
early_stopping_rounds = 50,
verbose=0)
if(count == 1){
best_params = bst_grid$params
best_score = bst_grid$best_score
count = count + 1
}
else if( bst_grid$best_score < best_score){
best_params = bst_grid$params
best_score = bst_grid$best_score
}
}
}
best_params
best_score
#fit XGBoost model and display training and testing data at each iteartion
#xgb.train(data = xgb_train, max.depth = 2, watchlist=watchlist, nrounds = 1000)
model_xgb = xgb.train(data = xgb_train,nthread = 2, eta = 0.35, max.depth = 3, nrounds = 1000, watchlist = watchlist, objective = "reg:squarederror", early_stopping_rounds = 50,  verbose = 1)
summary(model_xgb)
pred_xgb = predict(model_xgb, xgb_test)
cat("RMSE:", rmse(ytest, pred_xgb))
x = 1:length(ytest)                   # visualize the model, actual and predicted data
plot(x, ytest, col = "blue", type = "l", xlab='Stations',ylab = 'Scale parameter')
lines(x, pred_xgb, col = "red", type = "l")
legend("topleft", legend = c("y-test", "y-pred"),
col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
importance <- xgb.importance(feature_names = colnames(xgb_test),model = model_xgb)
print(xgb.ggplot.importance(importance_matrix = importance))
all_3 <- all_2 %>% dplyr::select(2,4,5,7,8,10,14,15,16,17)
#all_3$Count.x<-Count.pred
all_3[is.na(all_3)] <- 0
#make this example reproducible
set.seed(1)
#use 80% of dataset as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(all_3), replace=TRUE, prob=c(0.8,0.2))
train <- all_3[sample, ]
test <- all_3[!sample, ]
cat("RMSE:", rmse(test$Shape.y, test$Shape.x))
#Linear regression
reg_l<-lm(Shape.y~Longitude+Latitude+bio1+Shape.x, data=train)
summary(reg_l)
pred_reg <- predict(reg_l,test)
cat("RMSE:", rmse(test$Shape.y, pred_reg))
#tunning parameters
xtrain = data.matrix(train[,-3])
ytrain = train[,3]
xtest = data.matrix(test[,-3])
ytest = test[,3]
xgb_train = xgb.DMatrix(data = xtrain, label = ytrain)
xgb_test = xgb.DMatrix(data = xtest, label = ytest)
watchlist = list(train=xgb_train, test=xgb_test)
#create hyperparameter grid
max.depths = c(2, 3, 6, 9)
etas = c(0.2, 0.35, 0.4, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8)
best_params = 0
best_score = 0
count = 1
for( depth in max.depths ){
for( num in etas){
bst_grid = xgb.train(data = xgb_train,
max.depth = depth,
eta=num,
nthread = 2,
nround = 1000,
watchlist = watchlist,
objective = "reg:squarederror",
early_stopping_rounds = 50,
verbose=0)
if(count == 1){
best_params = bst_grid$params
best_score = bst_grid$best_score
count = count + 1
}
else if( bst_grid$best_score < best_score){
best_params = bst_grid$params
best_score = bst_grid$best_score
}
}
}
best_params
best_score
#fit XGBoost model and display training and testing data at each iteartion
#xgb.train(data = xgb_train, max.depth = 2, watchlist=watchlist, nrounds = 1000)
model_xgb = xgb.train(data = xgb_train,nthread = 2, eta = 0.6, max.depth = 2, nrounds = 1000, watchlist = watchlist, objective = "reg:squarederror", early_stopping_rounds = 50,  verbose = 1)
summary(model_xgb)
pred_xgb = predict(model_xgb, xgb_test)
cat("RMSE:", rmse(ytest, pred_xgb))
x = 1:length(ytest)                   # visualize the model, actual and predicted data
plot(x, ytest, col = "blue", type = "l", xlab='Stations',ylab = 'Scale parameter')
lines(x, pred_xgb, col = "red", type = "l")
legend("topleft", legend = c("y-test", "y-pred"),
col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
plot(train$scale.y)
plot(train$Shape.x)
all_3 <- all_2 %>% dplyr::select(2,4,5,7,8,10,14,15,16,17)
#all_3$Count.x<-Count.pred
all_3[is.na(all_3)] <- 0
#make this example reproducible
set.seed(2)
#use 80% of dataset as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(all_3), replace=TRUE, prob=c(0.8,0.2))
train <- all_3[sample, ]
test <- all_3[!sample, ]
cat("RMSE:", rmse(test$Shape.y, test$Shape.x))
plot(train$scale.y)
plot(train$shape.y)
plot(train$Shape.y)
#Linear regression
reg_l<-lm(Shape.y~Longitude+Latitude+bio1+Shape.x, data=train)
summary(reg_l)
pred_reg <- predict(reg_l,test)
cat("RMSE:", rmse(test$Shape.y, pred_reg))
GAM <- gam(Shape.y ~ s(Longitude,bs="bs")+s(Shape.x,bs="bs")+s(Latitude,bs='bs'), data = train ,method="REML")
summary(GAM)
pred <- predict.gam(GAM,test)
cat("RMSE:", rmse(test$Shape.y, pred))
#tunning parameters
xtrain = data.matrix(train[,-3])
ytrain = train[,3]
xtest = data.matrix(test[,-3])
ytest = test[,3]
xgb_train = xgb.DMatrix(data = xtrain, label = ytrain)
xgb_test = xgb.DMatrix(data = xtest, label = ytest)
watchlist = list(train=xgb_train, test=xgb_test)
#create hyperparameter grid
max.depths = c(2, 3, 6, 9)
etas = c(0.2, 0.35, 0.4, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8)
best_params = 0
best_score = 0
count = 1
for( depth in max.depths ){
for( num in etas){
bst_grid = xgb.train(data = xgb_train,
max.depth = depth,
eta=num,
nthread = 2,
nround = 1000,
watchlist = watchlist,
objective = "reg:squarederror",
early_stopping_rounds = 50,
verbose=0)
if(count == 1){
best_params = bst_grid$params
best_score = bst_grid$best_score
count = count + 1
}
else if( bst_grid$best_score < best_score){
best_params = bst_grid$params
best_score = bst_grid$best_score
}
}
}
best_params
best_score
#fit XGBoost model and display training and testing data at each iteartion
#xgb.train(data = xgb_train, max.depth = 2, watchlist=watchlist, nrounds = 1000)
model_xgb = xgb.train(data = xgb_train,nthread = 2, eta = 0.35, max.depth = 2, nrounds = 1000, watchlist = watchlist, objective = "reg:squarederror", early_stopping_rounds = 50,  verbose = 1)
summary(model_xgb)
pred_xgb = predict(model_xgb, xgb_test)
cat("RMSE:", rmse(ytest, pred_xgb))
x = 1:length(ytest)                   # visualize the model, actual and predicted data
plot(x, ytest, col = "blue", type = "l", xlab='Stations',ylab = 'Scale parameter')
lines(x, pred_xgb, col = "red", type = "l")
legend("topleft", legend = c("y-test", "y-pred"),
col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
all_3 <- all_2 %>% dplyr::select(2,4,5,7,8,10,14,15,16,17)
#all_3$Count.x<-Count.pred
all_3[is.na(all_3)] <- 0
#make this example reproducible
set.seed(3)
#use 80% of dataset as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(all_3), replace=TRUE, prob=c(0.8,0.2))
train <- all_3[sample, ]
test <- all_3[!sample, ]
cat("RMSE:", rmse(test$Shape.y, test$Shape.x))
#Linear regression
reg_l<-lm(Shape.y~Longitude+Latitude+bio1+Shape.x, data=train)
summary(reg_l)
pred_reg <- predict(reg_l,test)
cat("RMSE:", rmse(test$Shape.y, pred_reg))
GAM <- gam(Shape.y ~ s(Longitude,bs="bs")+s(Shape.x,bs="bs")+s(Latitude,bs='bs'), data = train ,method="REML")
summary(GAM)
pred <- predict.gam(GAM,test)
cat("RMSE:", rmse(test$Shape.y, pred))
#tunning parameters
xtrain = data.matrix(train[,-3])
ytrain = train[,3]
xtest = data.matrix(test[,-3])
ytest = test[,3]
xgb_train = xgb.DMatrix(data = xtrain, label = ytrain)
xgb_test = xgb.DMatrix(data = xtest, label = ytest)
watchlist = list(train=xgb_train, test=xgb_test)
#create hyperparameter grid
max.depths = c(2, 3, 6, 9)
etas = c(0.2, 0.35, 0.4, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8)
best_params = 0
best_score = 0
count = 1
for( depth in max.depths ){
for( num in etas){
bst_grid = xgb.train(data = xgb_train,
max.depth = depth,
eta=num,
nthread = 2,
nround = 1000,
watchlist = watchlist,
objective = "reg:squarederror",
early_stopping_rounds = 50,
verbose=0)
if(count == 1){
best_params = bst_grid$params
best_score = bst_grid$best_score
count = count + 1
}
else if( bst_grid$best_score < best_score){
best_params = bst_grid$params
best_score = bst_grid$best_score
}
}
}
best_params
best_score
#fit XGBoost model and display training and testing data at each iteartion
#xgb.train(data = xgb_train, max.depth = 2, watchlist=watchlist, nrounds = 1000)
model_xgb = xgb.train(data = xgb_train,nthread = 2, eta = 0.6, max.depth = 2, nrounds = 1000, watchlist = watchlist, objective = "reg:squarederror", early_stopping_rounds = 50,  verbose = 1)
summary(model_xgb)
pred_xgb = predict(model_xgb, xgb_test)
cat("RMSE:", rmse(ytest, pred_xgb))
x = 1:length(ytest)                   # visualize the model, actual and predicted data
plot(x, ytest, col = "blue", type = "l", xlab='Stations',ylab = 'Scale parameter')
lines(x, pred_xgb, col = "red", type = "l")
legend("topleft", legend = c("y-test", "y-pred"),
col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
#Linear regression
reg_l<-lm(Shape.y~Longitude+Latitude+bio12+Shape.x, data=train)
summary(reg_l)
pred_reg <- predict(reg_l,test)
cat("RMSE:", rmse(test$Shape.y, pred_reg))
GAM <- gam(Shape.y ~ s(Longitude,bs="bs")+s(Shape.x,bs="bs")+s(Latitude,bs='bs'), data = train ,method="REML")
summary(GAM)
pred <- predict.gam(GAM,test)
cat("RMSE:", rmse(test$Shape.y, pred))
GAM <- gam(Shape.y ~ s(Longitude,bs="bs")+s(Shape.x,bs="bs")+s(bio12,bs='bs'), data = train ,method="REML")
summary(GAM)
GAM <- gam(Shape.y ~ s(Longitude,bs="bs")+s(bio12,bs='bs'), data = train ,method="REML")
summary(GAM)
pred <- predict.gam(GAM,test)
cat("RMSE:", rmse(test$Shape.y, pred))
GAM <- gam(Shape.y ~ shape.x+s(Longitude,bs="bs")+s(bio12,bs='bs'), data = train ,method="REML")
GAM <- gam(Shape.y ~ Shape.x + s(Longitude,bs="bs")+s(bio12,bs='bs'), data = train ,method="REML")
summary(GAM)
GAM <- gam(Shape.y ~ s(Longitude,bs="bs")+s(bio12,bs='bs'), data = train ,method="REML")
summary(GAM)
pred <- predict.gam(GAM,test)
cat("RMSE:", rmse(test$Shape.y, pred))
#Linear regression
reg_l<-lm(Shape.y~Longitude+Latitude+bio12+Shape.x, data=train)
summary(reg_l)
pred_reg <- predict(reg_l,test)
cat("RMSE:", rmse(test$Shape.y, pred_reg))
GAM <- gam(Shape.y ~ s(Longitude,bs="bs")+s(bio12,bs='bs'), data = train ,method="REML")
summary(GAM)
pred <- predict.gam(GAM,test)
cat("RMSE:", rmse(test$Shape.y, pred))
#tunning parameters
xtrain = data.matrix(train[,-3])
ytrain = train[,3]
xtest = data.matrix(test[,-3])
ytest = test[,3]
xgb_train = xgb.DMatrix(data = xtrain, label = ytrain)
xgb_test = xgb.DMatrix(data = xtest, label = ytest)
watchlist = list(train=xgb_train, test=xgb_test)
#create hyperparameter grid
max.depths = c(2, 3, 6, 9)
etas = c(0.2, 0.35, 0.4, 0.5, 0.55, 0.6, 0.65, 0.7, 0.8)
best_params = 0
best_score = 0
count = 1
for( depth in max.depths ){
for( num in etas){
bst_grid = xgb.train(data = xgb_train,
max.depth = depth,
eta=num,
nthread = 2,
nround = 1000,
watchlist = watchlist,
objective = "reg:squarederror",
early_stopping_rounds = 50,
verbose=0)
if(count == 1){
best_params = bst_grid$params
best_score = bst_grid$best_score
count = count + 1
}
else if( bst_grid$best_score < best_score){
best_params = bst_grid$params
best_score = bst_grid$best_score
}
}
}
best_params
best_score
#fit XGBoost model and display training and testing data at each iteartion
#xgb.train(data = xgb_train, max.depth = 2, watchlist=watchlist, nrounds = 1000)
model_xgb = xgb.train(data = xgb_train,nthread = 2, eta = 0.6, max.depth = 2, nrounds = 1000, watchlist = watchlist, objective = "reg:squarederror", early_stopping_rounds = 50,  verbose = 1)
summary(model_xgb)
pred_xgb = predict(model_xgb, xgb_test)
cat("RMSE:", rmse(ytest, pred_xgb))
x = 1:length(ytest)                   # visualize the model, actual and predicted data
plot(x, ytest, col = "blue", type = "l", xlab='Stations',ylab = 'Scale parameter')
lines(x, pred_xgb, col = "red", type = "l")
legend("topleft", legend = c("y-test", "y-pred"),
col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
importance <- xgb.importance(feature_names = colnames(xgb_test),model = model_xgb)
print(xgb.ggplot.importance(importance_matrix = importance))
set.seed(1)
maxs <- apply(all_3, 2, max)
mins <- apply(all_3, 2, min)
scaled <- as.data.frame(scale(all_3, center = mins,
scale = maxs - mins))
train <- scaled[sample, ]
test <- scaled[!sample, ]
xtrain = as.matrix(train[,-3])
ytrain = as.matrix(train[,3])
xtest = as.matrix(test[,-3])
ytest = as.matrix(test[, 3])
xtrain = array(xtrain, dim = c(nrow(xtrain), dim(xtrain)[2], 1))
xtest = array(xtest, dim = c(nrow(xtest), dim(xtest)[2], 1))
dim(xtrain)
dim(xtest)
in_dim = c(dim(xtrain)[2:3])
print(in_dim)
model = keras_model_sequential() %>%
layer_conv_1d(filters = 128, kernel_size = 2,
input_shape = in_dim, activation = "relu") %>%
layer_conv_1d(filters = 64, kernel_size = 2,
input_shape = in_dim, activation = "relu") %>%
layer_flatten() %>%
layer_dense(units = 32) %>%
layer_dense(units = 16) %>%
layer_dense(units = 1, activation = "linear")
model %>% compile(loss = 'mse',
optimizer = 'adam',
metrics = list("mean_absolute_error")
)
model %>% summary()
model %>% fit(xtrain, ytrain, epochs = 100, batch_size=32, shuffle = FALSE, verbose = 0)
scores = model %>% evaluate(xtrain, ytrain, verbose = 0)
print(scores)
ypred = model %>% predict(xtest)
ypred <- ypred* (max(all_3$Shape.y) - min(all_3$Shape.y)) + min(all_3$Shape.y)
ytest <- (ytest) * (max(all_3$Shape.y) - min(all_3$Shape.y)) +min(all_3$Shape.y)
cat("RMSE:", rmse(ytest, ypred))
x_axes = seq(1:length(ypred))
plot(x_axes, ytest, ylim = c(min(ypred), max(ytest)),
col = "blue", type = "l", lwd = 2, ylab = "Shape parameter", xlab = 'Stations')
lines(x_axes, ypred, col = "red", type = "l", lwd = 2)
legend("topleft", legend = c("y-test", "y-pred"),
col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
load("C:/Users/vrizg/Desktop/Statistical Consulting/Estimates/Data_2000_2018.RData")
View(data_wet)
load("C:/Users/vrizg/Desktop/Statistical Consulting/Estimates/Data_2018.RData")
load("C:/Users/vrizg/Desktop/Statistical Consulting/Estimates/Data_2000_2018.RData")
View(all)
